\documentclass{harvardml}

% Authors: Mark Goldstein, Sandy Suh, Helen Wu
% January 6, 2018

% Adapated from CS281 Fall 2017 section 0 notes

% This tex file relies on
% the presence of two files:
% harvardml.cls and common.sty

\course{CS181-s18}
\assignment{CS181 Section \#0 Notes, v1.1}
\duedate{never}

\usepackage{url, enumitem}
\usepackage{amsfonts, amsmath, amsthm}
\usepackage{listings}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\theoremstyle{plain}
\usepackage[textsize=tiny]{todonotes}

% Some useful macros.
\newcommand{\given}{\,|\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\p}{\partial}
\newcommand{\mba}{\mathbf{a}}
\newcommand{\mbb}{\mathbf{b}}
\newcommand{\mbx}{\mathbf{x}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\boldw}{\mathbf{w}}
\newcommand{\mbxt}{\tilde{\mathbf{x}}}
\newcommand{\Sigmat}{\tilde{\Sigma}}
\newcommand{\mbz}{\mathbf{z}}
\newcommand{\mbw}{\mathbf{w}}
\newcommand{\mcN}{\mathcal{N}}
\newcommand{\mcP}{\mathcal{P}}
\newcommand{\eps}{\epsilon}
\newcommand{\trans}{\intercal}
\newcommand{\Ut}{\tilde{U}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\angstrom}{\textup{\AA}}
\renewcommand{\v}[1]{\mathbf{#1}}

\begin{document}

\section{Linear Algebra}
    \subsection{Scalars and Vectors}
		A \textbf{scalar} is a single element
		of a field. For example, the real number $s \in \R$
		is a scalar. We write scalars in lowercase.\\

		\noindent A \textbf{vector} of $n$ dimensions is an ordered
		collection of $n$ coordinates, where each coordinate is a scalar
		of the underlying field. An $n$-dimensional vector $\v v$ with
		real coordinates is an element of $\R^n$. Equivalently, the 
		coordinates specify as single point in an $n$-dimensional space. 
		By default, vectors will be columns and their transposes will be 
		rows. We write vectors in bold lowercase, and the vector itself 
		as a column of scalars:

		\begin{align*}
    		\mathbf{x} &= \begin{bmatrix}
           					x_{1} \\
           					x_{2} \\
           					\vdots \\
           					x_{n}
         				  \end{bmatrix}
  		\end{align*}


		\noindent Vectors may be scaled. $a\v x$ scales 
		each element of $\v x$ by scalar $a$. Vectors
		of the same dimension may be added coordinate-wise. 
		Vectors have both a \textbf{direction} and a
		\textbf{magnitude}. The magnitude, typically the
		\textbf{L2 norm}, of a vector can be computed as the
		square root of the sum of the squares of the coordinates:
		
		$$ ||\v x||_2 = \sqrt{\sum_{i=1}^{n} x_i^2} $$

		\noindent Refer to other vector norms
        such as the \textbf{L1}, \textbf{LP}, and
        \textbf{L}$\boldsymbol{\infty}$ norms. 
        Express the direction  as a vector of magnitude one. 
        Use $$ \frac{1}{||\mathbf{x}||_2}\mathbf{x} $$.
        
        \noindent An important product between vectors of the
                  same dimension is the \textbf{inner product} 
                  (also called dot product or scalar product).
                  For two vectors $\mathbf{u}$ and $\mathbf{v}$,
                  this is equal to $\sum_{i=1}^nu_iv_i = ||\mathbf{u}||_2 ||\mathbf{v}||_2 \cos \alpha$ where
		  $\alpha$ is the angle between $\v u$ and $\v v$. Note that
                  a vector $\mathbf{u}$ dotted with itself equals
                  the square of its L2 norm: $\langle \mathbf{u},
                  \mathbf{u} \rangle = ||\mathbf{u}||_2^2$. The 
                  \textbf{outer product} between two vectors is
                  the matrix $\mathbf{W}$ whose entries are
                  $w_{ij} = u_iv_j$.
   
    \subsection{Linear Independence}
        
        A set of non-zero vectors $\{\v v_1,...,\v v_n\}$ is 
		\textbf{linearly independent} if the equation 
		$c_1\v v_1 + c_2\v v_2 +,...,+ c_n\v v_n = \v 0$ 
        for scalars $c_1,...,c_n$ can only be satisfied by setting 
		$c_1,...,c_n$ all to $0$.

    \subsection{Spaces and Subspaces}

        A \textbf{vector space} $\mathcal{V}$ is a collection of 
		vectors that follow several axioms regarding the properties 
		of scaling and addition described above, and most importantly:
            \begin{itemize}
                \item closure under scaling: 
					  $\forall \v v \in \mathcal{V}$ 
					  and scalars $a$,
                      $ a \v v \in \mathcal{V}$
                \item closure under addition: 
					  $\forall \v u, 
					  \v v \in \mathcal{V},
                      (\v u + \v v) \in \mathcal{V}$
            \end{itemize}
        \noindent The set of vectors $\{\v v_1,...,\v v_n\}$ form an 
        \textbf{orthonormal basis} for $\mathcal{V}$ if 
        they are all unit vectors (normal) and if $\langle \v v_i, 
        \v v_j \rangle = 0,\forall i \neq j$ (orthogonal) where 
		$\langle,\rangle$ is the inner product. Let $\mathcal{S}$ 
		be a vector space. If all of these hold:
        \begin{itemize}
            \item $\mathcal{S} \subseteq \mathcal{V}$
            \item closure under scaling: $\forall \v u 
				  \in \mathcal{S}$ and scalars $a$, $a\v u 
				  \in \mathcal{S}$
            \item closure under addition: $\forall \v u, 
				  \v v \in \mathcal{S}, 
                  (\v u + \v v) \in \mathcal{S}$
        \end{itemize}
        \noindent then $\mathcal{S}$ is a \textbf{subspace} 
		of $\mathcal{V}$

    \subsection{Scalar, Vector, and Subspace Projection}
        For vectors $\v u, \v v \in \mathcal{V}$ and $\v v \neq \v 0$, the 
        \textbf{scalar projection} $a$ of $\v u$ onto $\v v$ is computed as:
            \begin{align*}
                a = \frac{\langle \v u, \v v \rangle}{||\v v||}
            \end{align*}
        \noindent Using this, the \textbf{vector projection} 
		$\v p$ of $\v u$ onto $\v v$ can be computed as:
            \begin{align*}
                a\big(\frac{1}{||\v v||}\v v\big) = 
                \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle} \v v
            \end{align*}
        \noindent This has the properties that $\langle \v u - \v p,
		\v p \rangle = \v 0$ and $\v u = \v p$ if and only if $\v u$ 
		is a scaled multiple of $\v v$.

        \noindent Finally (this is important for ML), it is possible
        to project a vector $\v u$ in a vector space $\mathcal{V}$ onto a
        subspace  $\mathcal{S}$ of $\mathcal{V}$. If the set of vectors
        $\{\v s_1,...,\v s_m\}$ form an orthonormal basis for $\mathcal{S}$,
        then the \textbf{subspace projection} $\v p$ of $\v u$ onto 
		$\mathcal{S}$ can be expressed as the sum of the projections of 
		$\v u$ onto each element of the basis of $\mathcal{S}$:
            \begin{align*}
                \v p = \sum_{i=1}^m 
				\frac{\langle \v u, \v s_i \rangle}
                {\langle \v s_i, \v s_i \rangle} \v s_i
            \end{align*}
        \noindent This has the properties that the vector 
		$\v u - \v p$ is orthogonal to all vectors in $\mathcal{S}$, 
		that $\v u = \v p$ if and only if $\v u \in \mathcal{S}$, and 
		that $\v p$ is the closest vector in $\mathcal{S}$ to $\v u$.
        $|| \v u - \v v || > || \v u - \v p ||, \forall \v v \neq \v p, 
		\v v \in \mathcal{S}$. A couple of connections: reconstruction 
		loss of dimensionality reduction and projection as conditional 
		probability.

    \subsection{Matrices}
		A \textbf{matrix} is a rectangular array of scalars.
		Primarily, an $n \times m$ matrix $\mathbf{A}$ is used to 
		describe a \textbf{linear transformation} from $m$ to $n$ 
		dimensions, where the matrix is an \textbf{operator}. If the 
		underlying field is $\R$, then $\mathbf{A} \in \R^{n \times m}$. 
		$A_{ij}$ is the scalar found at the $i^{th}$ row and $j^{th}$
		column. We write matrices in bold uppercase.
        A typical linear transformation looks like
            $\mathbf{y} = \mathbf{Ax}$ where $\mathbf{x} \in \R^m,
               \mathbf{y} \in \R^n, \mathbf{A} \in \R^{n \times m}$.
            What's linear? The property that $\mathbf{A}(\lambda_1\mathbf{u} 
			+ \lambda_2\mathbf{v}) = \lambda_1\mathbf{Au} +
            \lambda_2\mathbf{Av}$ for scalars $\lambda_1$ and $\lambda_2$.
			These notes do not go into the generalizations
			of many matrix properties from $\R$ to $\C$ 
			(e.g. transpose to conjugate transpose, symmetric to Hermitian).
    \subsection{Matrix Properties}
        \begin{itemize}
            \item $\mathbf{A}^\top$ is the \textbf{transpose} of
				  $\mathbf{A}$ and has $A^\top_{ji} = A_{ij}$.
            \item $\mathbf{A}$ is \textbf{symmetric} if 
				  $A_{ij} = A_{ji}$. That is, $\mathbf{A} = \mathbf{A}^\top$.
                  Only square matrices can be symmetric.
            \item $\mathbf{A}$ is said to be \textbf{orthogonal}
                  if its rows and its columns are orthogonal unit vectors.
                  Consequence: $\mathbf{A}^\top\mathbf{A} = 
							   \mathbf{A}\mathbf{A}^\top = \mathbf{I}$
				  where $\mathbf{I}$ is the \textbf{identity matrix}
				  (ones on the main diagonal and zeros elsewhere).
				  Orthogonal matrix $\mathbf{A}$
				  has $\mathbf{A}^\top = \mathbf{A}^{-1}$.
            \item \textbf{Diagonal} matrices have non-zero values on the main
                  diagonal and zeros elsewhere. Diagonal matrices are easy to 
                  take powers of. Under certain conditions a matrix may be 
                  diagonalized, see eigen-decomposition and SVD below.

            \item A matrix is \textbf{upper-triangular} if the only non-zero
                  values are on the diagonal or above (top right of matrix).
                  A matrix is \textbf{lower-triangular} if the only non-zero
                  values are on the diagonal or below (bottom right of matrix).

            \end{itemize}
    \subsection{Matrix Multiplication Properties}
			$\mathbf{A}\mathbf{B}$ is a valid \textbf{matrix product}
			if $\mathbf{A}$ is $p \times q$ and $\mathbf{B}$ is $q \times r$
			(left matrix has same number of columns as right matrix has rows).
				  There are many others important matrix products,
				  such as the element-wise \textbf{Hadamard} product
				  $\mathbf{A} \odot \mathbf{B}$ between matrices of the same
				  shape. The standard matrix product corresponds to the 
				  composition of operators.
			\begin{itemize}
				\item Generally not commutative: 
					  $\mathbf{A}\mathbf{B} \neq \mathbf{B}\mathbf{A}$
            	\item Left/Right Distributive over addition:
					  $\mathbf{A}(\mathbf{B} + \mathbf{C}) = 
					  \mathbf{AB} + \mathbf{AC}$.
				  	  $(\mathbf{A} + \mathbf{B})\mathbf{C} = 
					  \mathbf{AC} + \mathbf{BC}$.
				\item For some scalar $\lambda$:
					$ \lambda(\mathbf{AB}) = 
					(\lambda\mathbf{A})\mathbf{B} $ and
					$ (\mathbf{AB})\lambda = \mathbf{A}
					(\mathbf{B}\lambda) $, 
                    and all four are equal if 
					$\lambda$ is real or complex.
				\item transpose of product:
					   $(\mathbf{AB})^\top = 
						\mathbf{B}^\top\mathbf{A}^\top$ 
			\end{itemize}

    
    \subsection{Rank, Determinant, Inverse}
		
		The rank of a matrix $\mathbf{A}$ is the \textbf{dimension} 
        of the vector space spanned by its column vectors. A matrix 
    	is full rank if all its column vectors are linearly independent. 
        The same holds for row vectors. If $\mathbf{A}$ is $n \times m$, 
        then $rank(A) \leq min(n,m)$.\\

		\noindent The \textbf{determinant} of a square matrix is a 
		scalar quantity with various uses. Its computation differs 
		for square matrices of different sizes. The existence of a 
		matrix inverse depends on a non-zero determinant. 
		$det(\mathbf{A})$ is also the product 
        of the eigenvalues of $\mathbf{A}$. You may see the determinant
        denoted with single bars, e.g. $|\v X |$. However, the
		authors prefer $det(\v X)$. 
		Do not confuse $| \v X|$ with double bars 
		$||\v X||$, which typically denote a norm.\\
		
		\noindent The \textbf{inverse} $\mathbf{A}^{-1}$ of matrix operator 
		$\mathbf{A}$ ``undoes" $\mathbf{A}$ much like multiplying by 
        $\frac{1}{x}$ undoes multiplying by $x$. $\mathbf{A}^{-1}$ 
        only exists if $det(\mathbf{A}) \neq 0$. In general, matrix 
        inversion is a complicated operation, but special cases that 
        are easy to work with come up in the machine learning literature. 
        Often analytical solutions to systems depend on the existence of 
        the inverse of a matrix. $\mathbf{A}\mathbf{A}^{-1} = 
        \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$.\\
		
		\noindent The \textbf{Moore-Penrose pseudoinverse} $\mathbf{A}^+$ of 
        $\mathbf{A}$ is a generalization of the inverse to 
        non-square matrices, where $\mathbf{A}\mathbf{A}^+\mathbf{A} 
        = \mathbf{A}$. $\mathbf{A}\mathbf{A}^+$ may not be the general 
    	identity matrix but maps all column vectors of $\mathbf{A}$ to 
        themselves.

    \subsection{Eigen-Everything}
        Each linear operator (matrix) has some set of vectors in its domain
        that are simply mapped to a scaled version of the vector
        in the codomain. The operator preserves the direction of these vectors:
        $\mathbf{Ax} = \lambda\mathbf{x}$ for some scalar value $\lambda$. 
        In this case, $\lambda$ is an \textbf{eigenvalue} of $\mathbf{A}$ and 
        $\mathbf{x}$ is a corresponding \textbf{eigenvector}. These can be seen
        as the \textit{invariant directions} of the operator.\\

    	\noindent The eigenvectors of the empirical covariance 
        matrix of some data correspond to the directions
        of variance in the data. The eigenvectors with the
        largest associated eigenvalues correspond to the directions
        of highest variance in the data (by construction). 
		These directions may be linear combinations of the original 
		coordinates (features) of the data. See 
		\textbf{Principal Component Analysis} (PCA) 
        and dimensionality reduction.\\
                  
    	%\noindent \textbf{Low-rank approximations} reduce the
	%	dimensionality of a matrix so that it is useful for fast 
	%	computation or compression. In this problem one balances 
	%	the rank-minimization goal with reconstruction error between 
	%	the approximation and the true matrix. Of interest also are a low 
        %\textbf{matrix norm} and \textbf{sparsity}. One example is the entrywise 
	%	\textbf{Frobenius norm}.\\

		\noindent \textbf{Eigen-decomposition}:
		Let $\v A$ be an $n \times n$ full-rank
		matrix with $n$ linearly independent eigenvectors
		$\{\v q_i\}_{i=1}^n$. $\v A$ can be factored into
		$ \v A = \v Q \v \Lambda \v Q^{-1} $ where $\v Q$
		is $n \times n$ and has $\v q_i$ for its $i^{th}$
		column. $\v \Lambda$ is a diagonal matrix whose elements
		are the corresponding eigenvalues: $\Lambda_{ii} = \lambda_i$.
		If a matrix $\v A$ can be eigen-decomposed and none of its
		eigenvalues are $0$, then $\v A$ is \textbf{nonsingular} and
		its inverse is given by $\v A^{-1} = \v Q \v \Lambda^{-1}
		\v Q^{-1}$ with $\v \Lambda^{-1}_{ii} = \frac{1}{\lambda_i}$.\\

		\noindent \textbf{Singular Value Decomposition} is 
		a useful generalization of eigen-decomposition
		to rectangular matrices. Let $\v A$ be an $m \times n$ matrix. 
		Then $\v A$
		can be factored into $\v U \v \Sigma \v V^\top = 
		\v U \v \Sigma \v V^{-1}$ where
		\begin{itemize}
			\item $\v U$ is $m \times m$ and
		orthogonal. The columns of $\v U$ are
		the \textbf{left-singular vectors} of $\v A$.
			\item $\v \Sigma$ is an $m \times n$ diagonal
		matrix with non-negative real entries. The diagonal
		values $\sigma_i$ of $\v \Sigma$ are known as the
		\textbf{singular values} of $\v A$. These are also
		the square roots of the eigenvalues of $\v A^\top \v A$.
			\item $\v V$ is an $n
		\times n$ orthogonal matrix. The columns of $\v V$ are
		the \textbf{right-singular} vectors of $\v A$.
		\end{itemize}

    \subsection{Positive Definiteness}

        The symmetric matrix $\mathbf{A} \in \R^{n\times n}$ is 
        said to be \textbf{positive definite} if it satisfies the property 
        \begin{align*}
            \mathbf{x}^\top \mathbf{A} \mathbf{x} > 0
        \end{align*}
        and \textbf{positive semi-definite} if it satisfies
        \begin{align*}
            \mathbf{x}^\top \mathbf{A} \mathbf{x} \geq 0
        \end{align*}
        for every non-zero vector $\mathbf{x} \in \R^n$. 
        Positive definite matrices have all eigenvalues $>0$ 
        and positive semi-definite matrices have all eigenvalues $\geq0$.
    
\section{Calculus}
    \subsection{Differentiation}
        
            You should be familiar with single-variable differentiation,
            including properties like:

            \begin{align*}
                \text{Chain rule: } & \frac{d}{d x} f(g(x))= 
				f'(g(x))g'(x)\\
                \text{Product rule: }& \frac{d}{d x} f(x)g(x) = 
				f'(x)g(x) + f(x)g'(x)\\
                \text{Linearity: }& \frac{d}{d x} (af(x) + bg(x)) = 
				af'(x) + bg'(x)
            \end{align*}

            \noindent for scalars $a$ and $b$. In multivariable calculus, 
			a function may have some number of inputs (say $n$) and some 
			number of outputs (say $m$). In general, there is a partial 
			derivative for every input-output pair. This is called the 
			\textbf{Jacobian}. The $j^{th}$ column is made up of the partial 
			derivatives of $f_j$ (the $j^{th}$ output value of $\v f$) with 
			respect to all input elements, rows $i=1$ to $n$.
            \begin{align*}
                \frac{d \mathbf{f}(\mathbf{x})}{d \mathbf{x}} = 
				\begin{bmatrix}
                    \frac{\p f_1(\mathbf{x})}{\p x_1} & 
					\cdots & 
					\frac{\p f_m(\mathbf{x})}{\p x_1} \\
                    \vdots & \ddots & \vdots \\
                    \frac{\p f_1(\mathbf{x})}{\p x_n} & 
					\cdots & \frac{\p f_m(\mathbf{x})}{\p x_n}
                \end{bmatrix}
            \end{align*}
                
            \noindent If $f$ is scalar-valued, its derivative is a 
			column vector we call the \textbf{gradient vector} 
			(like a single column of the Jacobian):
            $$
                 \frac{d f(\mathbf{x})}{d \mathbf{x}}=\begin{bmatrix}
                 \frac{\p f(\mathbf{x})}{\p x_1} \\
                 \frac{\p f(\mathbf{x})}{\p x_2} \\
                 ... \\
                 \frac{\p f(\mathbf{x})}{\p x_n}
                 \end{bmatrix}
            $$

            \noindent The gradient vector points in the direction of 
			steepest ascent in $f(\mathbf{x})$ (actually, see diff. geo. for cases
			when this is not true). This is useful for 
			optimization.\\

            \noindent The \textbf{Hessian} matrix is like the Jacobian but with
            second-order derivatives. There are many interesting optimization
            topics related to the Hessian.\\

            \noindent A few important derivatives:
            \begin{align*}
                \frac{d \v x^\top \v a}{d \v x}
                    &= \frac{d \v a^\top \v x }{d \v x} = \v a\\
                \frac{d \v a^\top \v X \v b}{d \v X} &= \v a \v b^\top\\
                \frac{d \v a^\top \v X^\top \v b}{d \v X} &= \v b \v a^\top\\
                \frac{d \v a^\top \v X \v a}{d \v X} &=
                        \frac{d \v a^\top \v X^\top \v a}{d \v X} = 
                        \v a \v a^\top\\
                \frac{d \v X}{d X_{ij}} &= \v J^{ij} \quad \text{***}
            \end{align*}                
                
             *** $\v J$ is NOT the Jacobian, but rather, a matrix with
             all zeros except for a $1$ in the $i,j$ entry.\\
    
            \noindent Have you ever wondered how to differentiate
            the norm of a matrix? The eigenvalues? 
            For more, see the \textbf{Matrix Cookbook} 
            by Petersen and Pedersen (linked on course website).\\


    \subsection{Optimization}
            \textbf{Local Extrema}: Recall that the local extrema of a 
			single-variable function can be found by setting its derivative 
			to 0. The same is true here, using the condition 
			$\frac{d \mathbf{f}(\mathbf{x})}{d \mathbf{x}}= \mathbf{0}$. 
			However, this equation is often intractable. We can search for 
            local minima numerically using gradient-based methods.\\

            \noindent \textbf{Gradient Descent} (we will learn this in class): We start with an initial 
			guess at at a useful value for a parameter $\v w$: 
			$\mathbf{w}_0$. Then at each step $i$ we update our guess 
			by going in the direction of greatest descent of a loss 
			function (opposite the direction of the gradient vector):
                  \begin{align*}
                      \mathbf{w}_{i+1} = \mathbf{w}_i - \eta 
					  \frac{d f(\mathbf{w})}{d \mathbf{w}}
                  \end{align*}
        	where $\eta$ is a learning rate. We stop when the value of 
			the gradient is close to 0. \\

            %\noindent \textbf{Convexity}: A scalar-valued function 
			%$f(\mathbf{x})$ is \textbf{convex} 
            %if the line segment between any two points on the graph of 
            %the function lies above or on the graph. If the line always 
			%lies above, the function is strictly convex and the graph 
			%of $f$ is cup-shaped. This guarantees that $f$ has a unique 
			%global minimum, and that gradient descent will be able to 
			%find it. We will use gradient descent on non-convex 
			%functions anyway.
    
\section{Probability Theory}
    \subsection{Random Variables}
		A \textbf{random variable} can either be discrete or continuous. 
		A discrete random variable $X$ takes one of $m$ values from sample 
		space $\mcX$, each with a corresponding probability $p(x)$ for 
		$x\in \mcX$. $p(x)$ is the \textbf{probability mass function} of 
		$X$ and can also be written as $p_X(x)$. We say that $x \sim X$ 
		($x$ is sampled from $X$) when the value of $x$ is picked in 
		accordance with the distribution of $X$.\\

		\noindent A continuous random variable can take on a continuous 
		range of values. We use $p(x)$ or $p_X(x)$ for the 
		\textbf{probability density function} of a continuous random 
		variable. It's important to note that the probability of any one 
		exact value is zero. It's important to think of the function as
        assigning densities that behave like \textit{relative probabilities} 
		rather than absolute masses. Among other things, $p(x)$ can be 
		greater than 1.

    \subsection{Expectation}
            
		The \textbf{expected value} (or \textit{expectation, mean}) 
		of a random variable can be thought of as the ``weighted average'' 
		of the possible outcomes of the random variable. For discrete variables:
		\begin{align*}
			\E_{x \sim p(x)}[X] = \sum_{x \in \mathcal{X}} x \cdot p(x) \quad \quad \quad
		    \E[f(X)] = \sum_{x \in \mcX} f(x) p(x)
		\end{align*}
		\noindent For continuous variables:
		\begin{align*}
			\E[X] = \int_{\mcX} x \cdot p(x) dx \quad \quad \quad \E[f(X)] = \int_{\mcX} f(x) p(x) dx
		\end{align*}

		\noindent The most important property of expected values is the 
		\textbf{linearity of expectation}. For \textbf{any} two random 
		variables $X$ and $Y$ (regardless of independence)
		\begin{itemize}
			\item $\E[aX + bY + c] = a\E[X] + b\E[Y] + c$
			\item $\E[XY] = \E[X]\E[Y]$ under independence
		\end{itemize}

	\subsection{Variance}
		The variance of a random variable is its expected squared 
		deviation from its mean
			\begin{align*}
				var(X) &= \E[ (X - \E[X])^2 ] \\ 
					   &= \E[X^2] - (\E[X])^2
			\end{align*}

		\noindent Variance is a measure of the spread of a random 
		variable. High variance variables are more spread out. 
		Consider two normal distributions, one tall and skinny, 
		and the other shorter and wider.
		\begin{align*}
			var(aX + b) &= a^2var(X)
		\end{align*}

    \subsection{Joint Probability}

        The \textbf{joint probability} of $X=x$ and $Y=y$ is written 
		as $p(x,y)$ or $p_{XY}(x,y)$. For independent random variables 
		$X$ and $Y$ we have $p(x,y) = p(x)p(y)$. However, in the more 
		general case we must \textbf{condition}: $p(x,y) = p(x)p(y|x) = 
		p(y)p(x|y)$ (see next section). When you have a joint PMF or PDF 
		of two or more random variables, its a common situation to want 
		the \textbf{marginal distribution} of a single variable. For a 
		pair of random variables $X$ and $Y$, use the \textbf{sum rule}:

        \begin{align*}
            \text{Discrete: } p(x) &= \sum_{y \in \mcY} p(x,y)\\
            \text{Continuous: } p(x) &= \int_{y \in \mcY} p(x,y)\\
        \end{align*}

    \subsection{Conditional Probability}
		Receiving information about the value of a random variable $Y$ 
		can change the distribution of another variable $X$. We write the 
		new conditional random variable as $X|Y$, and the new conditional 
		distribution as $p(x|y)$. Manipulating the definition for the 
		joint probability of random variables that may be dependent, 
		we get:
			\begin{align*}
				p(x|y) &= \frac{p(x,y)}{p(y)}
			\end{align*}

        
        \noindent As mentioned above, when dealing with the joint probability 
        of several dependent variables, factor into chains of conditional 
        probabilities with the \textbf{product rule}:
        \begin{align*}
            p(x,y,z) 
            &= p(x)p(y|x)p(z|x,y) \\
            &= p(y)p(x|y)p(z|x,y) \\
            &= p(z)p(x|z)p(y|x,z) \\
            &= \text{etc...} \\
        \end{align*}
		See \url{http://colah.github.io/posts/2015-09-Visual-Information/}
		for some interesting visualizations of conditional probability 
		and information theory.\\

	\subsection{Bayes' Theorem} 
			This is a central theorem that we will use repeatedly in 
			this course, and is an extension of the product rule. 

			\begin{align*}
  				p(x | y) &= \frac{p(y | x) p(x)}{p(y)}
			\end{align*}

			\noindent Since we are conditioning on $y$, $y$ is held constant, 
			and that means $p(y)$ is just a normalization constant. 
			As a result, we often write the above property as

			\begin{align*}
  				p(x | y) \propto p(y | x) p(x)
			\end{align*}

			%\noindent To see this concretely in terms of machine learning
			%(we will learn this in class): 
			%say we observe data $D$, and we are interested in parameters 
			%$\mathbf{w}$.  We can write the \textbf{posterior distribution} 
			%of the parameters given data by using Bayes' theorem.

			%\begin{align*}
  			%	\underbrace{p(\boldw | D)}_{\text{posterior}}
    			%&= \frac{\overbrace{ p(D | \boldw) }^{\text{likelihood}} 
       			%\overbrace{ p(\boldw) }^{\text{prior}}}
       			%{\underbrace{p(D)}_{\text{evidence}}}
			%\end{align*}

			%\noindent Related to this, a maximum a posteriori (MAP) 
			%estimate for the parameter $\mathbf{w}$ is the value 
			%\[\argmax_\boldw p(\boldw | D)\] 
			%A maximum likelihood estimate (MLE) is the value 
            		%\[\argmax_{\boldw}p(D|\boldw)\]

	\subsection{Covariance}

        The \textbf{covariance} between two jointly distributed random 
		variables $X$ and $Y$ with finite variances is defined as the 
		expected product of their deviations from their individual 
		expected values. Intuitively, this asks: are $X$ and $Y$ 
		likely to tend above $\E[X]$ and $\E[Y]$ jointly (high covariance)? 
		Or does $X$ tend below $\E[X]$ while $Y$ tends above $\E[Y]$ and 
		vise versa (low covariance)?
        
        $$ \cov(X,Y) = \E[(X - \E[X])(Y - \E[Y])]$$

        \noindent When considering data in $n$ dimensions, 
        compute the $n \times n$ \textbf{covariance matrix} 
        (often denoted $\v \Sigma$), where $\v \Sigma_{ij} = 
        \cov(X_i,X_j)$ is the empirical covariance between the 
        $i^{th}$ and $j^{th}$ features.\\
        

		\noindent Properties of covariance: (supposing $X,Y,Z$ have mean 0 
                  and finite variances)
		\begin{itemize}
            \item Symmetric: $\cov(X,Y) = \cov(Y,X)$
            \item Positive Semi-definite: $\cov(X,X) \geq 0$
            \item $\cov(X,X) = 0$ implies $X$ always takes the same value, 
				  its mean
            \item Bilinear: $\cov(aX + bY,Z) = a\cov(X,Z) + b\cov(Y,Z)$
            \item Triangle Inequality: $|\cov(X,Y)| \leq \sqrt{\var(X)\var(Y)}$
		\end{itemize}

	\subsection{Conditional Expectation and Conditional Variance}
		It is common to determine the expectation and variance of a variable. 
		If $X$ and $Y$ are random variables, then $\E[X|Y]$ is a random 
		variable too, because it can take on several values depending on $Y$. 
		$\E[X|Y=y]$ is the expected (or average) value of the random variable 
		$X$ given a particular observed value of $Y$. This is called the 
		\textbf{conditional expectation} of $X$ given $Y=y$.\\
	
		\noindent Similarly, we can define \textbf{conditional variance} as
		\begin{align*}
            var(X|Y) = \E[(X - \E[X|Y])^2 | Y] = \E[X^2|Y] - \E[X|Y]^2
		\end{align*}
		
		\noindent \textbf{Adam's law} (law of iterated expectations) gives
		\begin{align*}
			\E[X]=\E[\E[X|Y]]
		\end{align*}

		\noindent \textbf{Eve's Law} (or law of total variance) is the 
		analogous case for variance
		\begin{align*}
  			\var[X] = \E[ \var[X|Y] ] + \var[ \E[X | Y] ]
		\end{align*}



	\subsection{Gaussians}
		\subsubsection{Univariate PDF}
			\begin{align*}
				\mathcal{N}(x; \mu, \sigma^2) = 
				\frac{1}{\sqrt{2\pi}\sigma} 
				\exp\left( -\frac{1}{2\sigma^2} (x - \mu)^2 \right)
			\end{align*}

		\begin{itemize}
			\item If $X,Y$ are independent normals then $X + Y \sim 
				  \mathcal{N}(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)$
			\item $aX+b \sim \mathcal{N}(a\mu + b, a^2 \sigma^2)$
			\item Any PDF proportional to $\exp(ax^2 + bx + c)$ 
				  must be a Gaussian PDF.
		\end{itemize}

		\subsubsection{Multivariate PDF}
		Given dimension $m$, mean vector $\v \mu \in \R^m$,
        and covariance matrix $\v \Sigma \in \R^{m \times m}$,
		\begin{align*}
            \mathcal{N}(\v x ; \v \mu, \v \Sigma) = 
                        \frac{1}{det(2\pi\v \Sigma)^{1/2}}
			            \exp \left( -\frac{1}{2} (\v x - \v \mu)^T 
						\v \Sigma^{-1} 
                        (\v x - \v \mu) \right)
		\end{align*}

		\noindent Note: there are many ways to write the MVN PDF. 
		You may notice the absence of $m$ in the coefficient. 
		This works because the $2\pi$ distributes nicely over 
		$\v \Sigma$ in the determinant.

\end{document}

